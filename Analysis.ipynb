{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make sure you have the Selenium library installed. You can install it using pip: pip install selenium\n",
    "#make sure to install pip install pymongo[srv]\n",
    "from splinter import Browser\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pymongo import MongoClient\n",
    "from selenium import webdriver \n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from urllib.parse import quote_plus\n",
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus \n",
    "\n",
    "# Initialize browser\n",
    "browser = Browser('chrome', executable_path='C:\\\\chromedriver_win32\\\\chromedriver.exe', headless=False)\n",
    "\n",
    "# List of NFL teams\n",
    "teams = ['arizona-cardinals', 'atlanta-falcons', 'baltimore-ravens', 'buffalo-bills', \n",
    "         'carolina-panthers', 'chicago-bears', 'cincinnati-bengals', 'cleveland-browns', \n",
    "         'dallas-cowboys', 'denver-broncos', 'detroit-lions', 'green-bay-packers', \n",
    "         'houston-texans', 'indianapolis-colts', 'jacksonville-jaguars', 'kansas-city-chiefs', \n",
    "         'las-vegas-raiders', 'los-angeles-chargers', 'los-angeles-rams', 'miami-dolphins', \n",
    "         'minnesota-vikings', 'new-england-patriots', 'new-orleans-saints', 'new-york-giants', \n",
    "         'new-york-jets', 'philadelphia-eagles', 'pittsburgh-steelers', 'san-francisco-49ers', \n",
    "         'seattle-seahawks', 'tampa-bay-buccaneers', 'tennessee-titans', 'washington-football-team']\n",
    "all_data = []\n",
    "for year in range(2020, 2024):\n",
    "    for team in teams:\n",
    "        # Visit the page\n",
    "        url = f\"https://www.spotrac.com/nfl/{team}/cap/{year}\"\n",
    "        browser.visit(url)\n",
    "\n",
    "        # Create BeautifulSoup object; parse with 'html.parser'\n",
    "        html = browser.html\n",
    "        soup = BeautifulSoup(html, 'html.parser')\n",
    "\n",
    "        # Find the correct table with Active Players\n",
    "        all_tables = soup.find_all('table')\n",
    "        active_table = None\n",
    "        for table in all_tables:\n",
    "            ths = table.find_all('th')\n",
    "            for th in ths:\n",
    "                if 'Active Players' in th.text:\n",
    "                    active_table = table\n",
    "                    break\n",
    "            if active_table is not None:\n",
    "                break\n",
    "\n",
    "        # Retrieve all elements that contain player salary information\n",
    "        players = active_table.find_all('tr')\n",
    "\n",
    "        # Iterate through each player\n",
    "        for player in players:\n",
    "            # Use Beautiful Soup's find() method to navigate and retrieve attributes\n",
    "            name_tag = player.find('td', class_='player')\n",
    "            if name_tag is not None:\n",
    "                a_tag = name_tag.find('a')\n",
    "                if a_tag is not None:\n",
    "                    name = a_tag.text\n",
    "                    # Find all td tags within the player's row\n",
    "                    all_tds = player.find_all('td')\n",
    "                    if len(all_tds) >= 12:  # There should be at least 12 td tags if the player row is valid\n",
    "                        position = all_tds[1].find('span').text\n",
    "                        cap_hit = all_tds[2].find('span').text.strip()\n",
    "                        base_salary = all_tds[3].find('span').text.strip()\n",
    "                        cap_percentage = all_tds[11].text.strip()\n",
    "                        \n",
    "                        player_data = {\n",
    "                            \"year\": year,\n",
    "                            \"team\": team,\n",
    "                            \"name\": name,\n",
    "                            \"position\": position,\n",
    "                            \"cap_hit\": cap_hit,\n",
    "                            \"base_salary\": base_salary,\n",
    "                            \"cap_percentage\": cap_percentage\n",
    "                        }\n",
    "                        all_data.append(player_data)\n",
    "\n",
    "# Convert to a DataFrame and export to a csv file\n",
    "df = pd.DataFrame(all_data)\n",
    "\n",
    "# Reset the index to start from 1 and name the index column as \"index\"\n",
    "df.index = df.index + 1\n",
    "df.index.name = \"index\"\n",
    "\n",
    "\n",
    "# Close the browser\n",
    "browser.quit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\binhd\\AppData\\Local\\Temp\\ipykernel_31144\\3213778602.py:34: DeprecationWarning: find_element_by_* commands are deprecated. Please use find_element() instead\n",
      "  dropdown = driver.find_element_by_id('range')\n",
      "C:\\Users\\binhd\\AppData\\Local\\Temp\\ipykernel_31144\\3213778602.py:37: DeprecationWarning: find_element_by_xpath is deprecated. Please use find_element(by=By.XPATH, value=xpath) instead\n",
      "  option = dropdown.find_element_by_xpath(f\"//option[@value='yearly_{year}']\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def parse_win_loss_record(record):\n",
    "    # Split the win-loss-tie record and extract win, loss, and tie values\n",
    "    parts = record.split('-')\n",
    "    if len(parts) == 3:\n",
    "        win, loss, tie = parts\n",
    "        return int(win), int(loss), int(tie)\n",
    "    else:\n",
    "        return None, None, None\n",
    "\n",
    "def scrape_win_loss_records():\n",
    "    base_url = \"https://www.teamrankings.com/nfl/trends/win_trends/\"\n",
    "    years = range(2010, 2023)\n",
    "\n",
    "    # Initialize lists to store the extracted data\n",
    "    years_list = []\n",
    "    team_names = []\n",
    "    wins = []\n",
    "    losses = []\n",
    "    ties = []\n",
    "\n",
    "    # Start the ChromeDriver service\n",
    "    service = Service('C:\\\\chromedriver_win32\\\\chromedriver.exe')  # Replace with the actual path to chromedriver\n",
    "    service.start()\n",
    "\n",
    "    # Create a new instance of the Chrome driver\n",
    "    driver = webdriver.Chrome(service=service)\n",
    "\n",
    "    # Open the URL in Chrome\n",
    "    driver.get(base_url)\n",
    "\n",
    "    # Loop through each year\n",
    "    for year in years:\n",
    "        # Find the dropdown element to select the year\n",
    "        dropdown = driver.find_element_by_id('range')\n",
    "\n",
    "        # Find the option corresponding to the current year\n",
    "        option = dropdown.find_element_by_xpath(f\"//option[@value='yearly_{year}']\")\n",
    "        option.click()\n",
    "\n",
    "        # Wait for a short duration for the page to load\n",
    "        time.sleep(2)\n",
    "\n",
    "        # Get the page source after selecting the year\n",
    "        page_source = driver.page_source\n",
    "\n",
    "        # Parse the HTML content using BeautifulSoup\n",
    "        soup = BeautifulSoup(page_source, 'html.parser')\n",
    "\n",
    "        # Find the table containing the win-loss records\n",
    "        table = soup.find('table', class_='tr-table')\n",
    "\n",
    "        if table:\n",
    "            # Loop through each row in the table and extract the team name and win-loss record\n",
    "            for row in table.find_all('tr'):\n",
    "                columns = row.find_all('td')\n",
    "                if len(columns) == 5:  # Ensure we have a valid row with data\n",
    "                    team_name = columns[0].text.strip()\n",
    "                    win_loss_record = columns[1].text.strip()\n",
    "\n",
    "                    # Split win-loss-tie record into separate columns\n",
    "                    win, loss, tie = parse_win_loss_record(win_loss_record)\n",
    "\n",
    "                    years_list.append(year)\n",
    "                    team_names.append(team_name)\n",
    "                    wins.append(win)\n",
    "                    losses.append(loss)\n",
    "                    ties.append(tie)\n",
    "\n",
    "            # Wait for a short duration before fetching data for the next year\n",
    "            time.sleep(1)\n",
    "        else:\n",
    "            print(f\"No data found for the year {year}\")\n",
    "\n",
    "    # Close the ChromeDriver\n",
    "    driver.quit()\n",
    "\n",
    "    # Combine data into a list of tuples\n",
    "    data = list(zip(years_list, team_names, wins, losses, ties))\n",
    "\n",
    "    # Create a Pandas DataFrame\n",
    "    df_nfl_score = pd.DataFrame(data, columns=['Year', 'Team', 'Win', 'Loss', 'Tie'])\n",
    "    # Return the DataFrame\n",
    "    return df_nfl_score\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "# Example usage:\n",
    "df_nfl_score = scrape_win_loss_records()\n",
    "# Mapping of names from nfl_scores to nfl_salaries\n",
    "team_name_mapping = {\n",
    "    'Baltimore': 'baltimore-ravens',\n",
    "    'San Francisco': 'san-francisco-49ers',\n",
    "    'Kansas City': 'kansas-city-chiefs',\n",
    "    'Green Bay': 'green-bay-packers',\n",
    "    'New Orleans': 'new-orleans-saints',\n",
    "    'New England': 'new-england-patriots',\n",
    "    'Seattle': 'seattle-seahawks',\n",
    "    'Minnesota': 'minnesota-vikings',\n",
    "    'Houston': 'houston-texans',\n",
    "    'Buffalo': 'buffalo-bills',\n",
    "    'Tennessee': 'tennessee-titans',\n",
    "    'LA Rams': 'los-angeles-rams',\n",
    "    'Philadelphia': 'philadelphia-eagles',\n",
    "    'Dallas': 'dallas-cowboys',\n",
    "    'Pittsburgh': 'pittsburgh-steelers',\n",
    "    'Chicago': 'chicago-bears',\n",
    "    'Atlanta': 'atlanta-falcons',\n",
    "    'Las Vegas': 'las-vegas-raiders',\n",
    "    'Tampa Bay': 'tampa-bay-buccaneers',\n",
    "    'Denver': 'denver-broncos',\n",
    "    'NY Jets': 'new-york-jets',\n",
    "    'Indianapolis': 'indianapolis-colts',\n",
    "    'Cleveland': 'cleveland-browns',\n",
    "    'Jacksonville': 'jacksonville-jaguars',\n",
    "    'Arizona': 'arizona-cardinals',\n",
    "    'LA Chargers': 'los-angeles-chargers',\n",
    "    'Miami': 'miami-dolphins',\n",
    "    'Carolina': 'carolina-panthers',\n",
    "    'NY Giants': 'new-york-giants',\n",
    "    'Detroit': 'detroit-lions',\n",
    "    'Washington': 'washington-football-team',\n",
    "    'Cincinnati': 'cincinnati-bengals',\n",
    "}\n",
    "\n",
    "# Replace team names in nfl_scores DataFrame\n",
    "df_nfl_score['Team'] = df_nfl_score['Team'].map(team_name_mapping)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pymongo.results.InsertManyResult at 0x2674a8ef600>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "username = quote_plus(\"binhdole\")\n",
    "password = quote_plus(\"1Nnov@t1on\")\n",
    "\n",
    "# Connection\n",
    "uri = f\"mongodb+srv://{username}:{password}@cluster0.dupn17e.mongodb.net/mydatabase\"\n",
    "client = MongoClient(uri)\n",
    "db = client.mydatabase\n",
    "\n",
    "# Convert DataFrame to dict\n",
    "\n",
    "data_nfl_scores = df_nfl_score.to_dict('records')\n",
    "\n",
    "# Choose the collections\n",
    "\n",
    "collection_scores = db['nfl_scores']\n",
    "\n",
    "# Insert the data\n",
    "\n",
    "collection_scores.insert_many(data_nfl_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (384699840.py, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 13\u001b[1;36m\u001b[0m\n\u001b[1;33m    with open('C:\\Users\\binhd\\Documents\\Sports_Project\\data.json') as json_file:\u001b[0m\n\u001b[1;37m              ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "from pymongo import MongoClient\n",
    "from urllib.parse import quote_plus\n",
    "import json\n",
    "\n",
    "# MongoDB connection\n",
    "username = quote_plus(\"binhdole\")\n",
    "password = quote_plus(\"1Nnov@t1on\")\n",
    "uri = f\"mongodb+srv://{username}:{password}@cluster0.dupn17e.mongodb.net/mydatabase\"\n",
    "client = MongoClient(uri)\n",
    "db = client.mydatabase\n",
    "\n",
    "# Read data from JSON file\n",
    "with open(r'C:\\Users\\binhd\\Documents\\Sports_Project\\data.json') as json_file:\n",
    "    data_nfl_salaries = json.load(json_file)\n",
    "\n",
    "\n",
    "# Choose the collection\n",
    "collection_salaries = db['nfl_salaries']\n",
    "\n",
    "# Insert the data\n",
    "collection_salaries.insert_many(data_nfl_salaries)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cartopy_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
